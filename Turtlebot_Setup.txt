		ANLEITUNG ZUM ERSTELLEN VON ROS AUF EINEM TURTLEBOT3 (RASPBERRY PI3)
		
		
	
	Git Anmeldedaten: 	E-Mail: baau1001@stud.hs-kl.de
				PW: Mechatronischesprojekt_2025
				Username: RoboProjekt
				
		

	-Ubuntu Server 22.04.5 LTS x64 mit 'rpi-imager' auf SD Karte schreiben
	-SD Karte in Pi und booten lassen
	
	-Verbinden mit: 'ssh pi@ip.adresse' (unser Fall 192.168.1.116)
	-Ubuntu updaten: 'sudo apt update' 'sudo apt upgrade'
	
	-Swap Installieren: 'sudo apt-get install dphys-swapfile' 
	-Swap stoppen: 'sudo swapoff -a'
	-Swap Einrichten: 'sudo nano /etc/dphys-swapfile' 
			  Sieht in etwa so aus: 
				# /etc/dphys-swapfile - Configuration file for dphys-swapfile
				# See /usr/share/dphys-swapfile/dphys-swapfile for full info.
	
				# the location of the swapfile, default is /var/swap
				CONF_SWAPFILE=/var/swap
	
				# set size to 1024 MB (1 GB)
				CONF_SWAPSIZE=1024 (unser Wert: 3000)
	
				# set size limit (if needed), 0 means no limit
				CONF_MAXSWAP=2048 (unser Wert: 5000 oder 0)
	
				# if you want to use a fixed size instead of dynamic, set this to 1	
				CONF_SWAPFACTOR=2 (unser Wert: 1)
	
				# enable/disable swapfile (1 = enable, 0 = disable)
				CONF_ENABLE=1
	
	-Swap starten: 'sudo swapon -a'
	
	-Ros Installieren: 
		--sudo apt install software-properties-common curl gnupg2 -y
		--sudo add-apt-repository universe
		--sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key \-o /usr/share/keyrings/ros-archive-keyring.gpg
		--echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] http://packages.ros.org/ros2/ubuntu $(. /etc/os-release && echo $UBUNTU_CODENAME) main" \ | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null
		--sudo apt update
		--sudo apt upgrade
		
		--sudo apt install ros-humble-ros-base -y 
		ODER
		--sudo apt install ros-humble-desktop (einschließlich Rviz, TurtleSim, Navigation2, Demo-Nodes und vielem mehr)
		
	-Sourcen von ROS2: 'nano ~/.bashrc' und 'source /opt/ros/humble/setup.bash' am Ende einfügen. mit strg+O speichern und mit strg+x schließen
	
	TurtleBot3 Burger einrichten:
		--sudo apt install python3-argcomplete python3-colcon-common-extensions build-essential libboost-system-dev libudev-dev -y
		--sudo apt install ros-humble-hls-lfcd-lds-driver ros-humble-dynamixel-sdk ros-humble-turtlebot3-msgs -y
		--mkdir -p ~/turtlebot3_ws/src && cd ~/turtlebot3_ws/src
		--git clone -b humble https://github.com/ROBOTIS-GIT/turtlebot3.git
		--git clone -b humble https://github.com/ROBOTIS-GIT/ld08_driver.git
		
	-eventuell nochmal 'source ~/.bashrc'
	
		--colcon build --symlink-install 
		ODER
		--colcon build	
		
		
----------------stand (21.05.2025 16:09)(Turtlebot3 Burger mit... pdf Seite2. Punkt 7)----------
		
		
	OPENCR INSTALLATION
	-sudo dpkg --add-architecture armhf
	-sudo apt-get update
	-sudo apt-get install libc6:armhf
	-export OPENCR_PORT=/dev/ttyACM0
	-export OPENCR_MODEL=burger
	-rm -rf ./opencr_update.tar.bz2
	-wget https://github.com/ROBOTIS-GIT/OpenCR-Binaries/raw/master/turtlebot3/ROS2/latest/opencr_update.tar.bz2
	-tar -xvf opencr_update.tar.bz2
	-cd ./opencr_update
	-./update.sh $OPENCR_PORT $OPENCR_MODEL.opencr
	
	TURTLEBOT BRINGUP/START
	-ssh pi@ip-adresse
	-export TURTLEBOT3_MODEL=burger
	-ros2 launch turtlebot3_bringup robot.launch.py
	
	KEYBOARD GESTEUERTE NUTZUNG
	Neues Terminal mit ssh:
	-export TURTLEBOT3_MODEL=burger
	-ros2 run turtlebot3_teleop teleop_keyboard
	
	KARTOGRAFIEREN MIT RVIZ AUF PC
	auf Pi: 'export TURTLEBOT3_MODEL=burger' danach 'ros2 launch turtlebot3_bringup robot.launch.py'
	auf PC: 'export TURTLEBOT3_MODEL=burger' danach 'ros2 launch turtlebot3_cartographer cartographer.launch.py'	
	
	SENDEN VON ORDNERN AN TURTLEBOT MIT SSH
	scp -r ~/mein/ordner pi@ip-adress:~/ziel/ordner/
	(scp -r ~/Schreibtisch/Turtlebot_Setup.txt pi@192.168.1.116:~/)
	
	LIVESTREAM DURCH PI KAMERA
	-sudo apt install mjpg-streamer
	-cd ~/mjpg_streamer/src/mjpg-streamer/mjpg-streamer-experimental
	./mjpg_streamer -i "./input_uvc.so -d /dev/video0 -r 640x480 -f 15" -o "./output_http.so -p 8080 -w ./www"
	Auf PC/Browser: "pi.ip.adresse:8080" (bei uns: "192.168.1.116:8080")
	
	ROS2 KAMERA AKTIVIEREN
	-sudo apt install libraspberrypi-bin v4l-utils ros-humble-v4l2-camera
	-sudo apt install ros-humble-image-transport-plugins
	-groups Es sollte einen Ordner video geben
	-sudo apt-get install raspi-config (Wenn noch nicht vorhanden)
	-sudo raspi-config
	--Interface Options (Legacy Camera enablen, SPI enablen, I2C enablen mit [TAB] auf finished wechseln)
	-vcgencmd get_camera (supported=1 detected=1)
	-tmux
	In tmux session: (Wird unabhängig von SSH ausgeführt)
	--ros2 run v4l2_camera v4l2_camera_node --ros-args
	[strg + B] anschließend [C] für neues tmux fenster
	--ros2 topic list Hier müsste dann auch /camera_info und /image_raw/... erscheinen
	(tmux nach beenden der ssh mit 'tmux ls' und 'tmux attach -t 0' wieder öffnen)

----------------stand (22.05.2025 17:09)--------------------------------------------------------
				
				
	SPRACHERKENNUNG
	-AUF PI
	--wget https://alphacephei.com/vosk/models/vosk-model-small-de-0.15.zip
	--unzip vosk-model-small-de-0.15.zip
	-AUF LAPTOP
	-pip install vosk sounddevice
	-Herunterladen von vosk-model-small-de-0.15
	-Erstellen von voice_control.py:
		import rclpy
		from rclpy.node import Node
		from geometry_msgs.msg import Twist
	
		import sounddevice as sd
		import queue
		import json
		import vosk
		import sys

		class VoiceControlNode(Node):
		    def __init__(self):
		        super().__init__('voice_control_node')
		        self.pub = self.create_publisher(Twist, 'cmd_vel', 10)

		        model_path = r"/home/basti/Schreibtisch/vosk-model-small-de-0.15"
		        self.get_logger().info(f"Lade Vosk-Modell von: {model_path}")
		        self.model = vosk.Model(model_path)
	
		        self.q = queue.Queue()
		        self.twist = Twist()

		        # Mikrofon-Gerät ID hier anpassen, oder None für Standard
		        self.device_id = None

		        self.stream = sd.RawInputStream(samplerate=16000, blocksize=8000, dtype='int16',
                                        channels=1, callback=self.audio_callback,
                                        device=self.device_id)
		        self.stream.start()

		        self.rec = vosk.KaldiRecognizer(self.model, 16000)

        		self.get_logger().info("Sprachsteuerung gestartet - sag: vorwärts, rückwärts, links, rechts, stopp")

        		self.timer = self.create_timer(0.1, self.timer_callback)
	
		    def audio_callback(self, indata, frames, time, status):
		        if status:
		            self.get_logger().warn(f"Sounddevice Status: {status}")
		        self.q.put(bytes(indata))

		    def timer_callback(self):
		        while not self.q.empty():
		            data = self.q.get()
		            if self.rec.AcceptWaveform(data):
		                result = json.loads(self.rec.Result())
		                text = result.get("text", "")
		                if text:
		                    self.get_logger().info(f"Erkannt: {text}")
        		            self.handle_command(text)

    		def handle_command(self, text):
		        # Reset Bewegung
		        self.twist.linear.x = 0.0
		        self.twist.angular.z = 0.0

		        if "vorwärts" in text:
		            self.twist.linear.x = 0.2
		        elif "rückwärts" in text:
		            self.twist.linear.x = -0.2
		        elif "links" in text:
		            self.twist.angular.z = 0.5
        		elif "rechts" in text:
	       		     self.twist.angular.z = -0.5
	       		 elif "halt" in text or "halt" in text:
	       		     # Keine Bewegung
		            pass
		        else:
		            # Unbekannter Befehl, nichts tun
			    return

		        self.pub.publish(self.twist)
		        self.get_logger().info(f"Bewegung gesendet: linear.x={self.twist.linear.x}, angular.z={self.twist.angular.z}")

		def main(args=None):
		    rclpy.init(args=args)
		    node = VoiceControlNode()
		    try:
		        rclpy.spin(node)
		    except KeyboardInterrupt:
		        pass
		    finally:
		        node.stream.stop()
		        node.stream.close()
		        node.destroy_node()
       			rclpy.shutdown()

		if __name__ == "__main__":
		    main()
	
	
	-Ende SPRACHERKENNUNG
	-Zum Starten 'ros2 launch turtlebot3_bringup robot.launch.py' auf dem pi und 'python3 voice_control.py' im '~/Schreibtisch' Verzeichnis starten
	

	
----------------stand (23.05.2025 16:00)--------------------------------------------------------
	
	
	EINFÜGEN AUTORACE CAMERA NODE (UND EIGENE)
	(Auf Pi)
	-cd ~/turtlebot3_ws/src/
	-git clone https://github.com/ROBOTIS-GIT/turtlebot3_autorace.git
	-cd ~/turtlebot3_ws && colcon build --symlink-install
	-sudo apt install ros-humble-image-transport ros-humble-cv-bridge ros-humble-vision-opencv python3-opencv libopencv-dev ros-humble-image-pipeline
	(-export TURTLEBOT3_MODEL=burger_cam)
	LAUNCH:
	 -ros2 launch turtlebot3_autorace_camera FILENAME
	FILENAME in [~/turtlebot3_ws/install/turtlebot3_autorace_camera/share/turtlebot3_autorace_camera/launch] und [~/turtlebot3_ws/src/turtlebot3_autorace/turtlebot3_autorace_camera/launch \\hier nicht bootable aber sichtbar]
	(bei uns ros2 launch turtlebot3_autorace_camera live_camera_launch.py)
	EIGENE FILES IM GIT
	
	
	DARKNET UND YOLOv4-TINY
	DARKNET
	-git clone https://github.com/AlexeyAB/darknet.git ~/darknet
	-cd ~/darknet
	-nano Makefile
	--GPU=0
          CUDNN=0
          CUDNN_HALF=0
          OPENCV=1
         ([strg]+O, [strg]+X)
        -make -j4
        YOLOv4-TINY
	-wget https://github.com/AlexeyAB/darknet/releases/download/yolov4/yolov4-tiny.weights
	TO RUN:
	-cd ~/darknet
	-./darknet detector demo cfg/coco.data cfg/yolov4-tiny.cfg yolov4-tiny.weights -c 0
	
	
----------------stand (04.06.2025 14:43)--------------------------------------------------------
	
	
	KI FÜR TÜREN ANLERNEN
	-download von roboflow yolo-aiv2 Computer Vision Project (https://universe.roboflow.com/yolo-ai-fubov/yolo-aiv2)
	-installation über downloadlink (Download dataset, Format YOLO Darknet, Show download code, Terminal (Bei uns: curl -L "https://universe.roboflow.com/ds/DRBvdKiREx?key=hsMtANmbdg" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip))
	-installieren tiny weights:
	--cd ~/darknet
	--wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.conv.29
	-Erstellen Ordner für Dataset:
	-- ~/darknet/data: mkdir yolo-aiv2
	-Kopieren des Dataset(Ordner 'train' und 'valid' nach ~/darknet/data/yolo-aiv2)
	-Erstellen von Files in yolo-aiv2:
	--nano obj.data:
	classes = 30
	train = data/yolo-aiv2/train.txt
	valid = data/yolo-aiv2/valid.txt
	names = data/yolo-aiv2/obj.names
	backup = backup/
	--nano obj.names:
	7 segment
	IC
	ID
	LCD
	NI Elvis
	PCB
	Voltage Regulator
	aircon
	arduino
	breadboard
	button
	capacitor	
	ceiling fan
	chassis	
	door
	keyboard
	led
	mobile phone
	monitor
	monoblock
	mouse
	multimeter
	person
	resistor
	table
	tie
	transistor
	trash bin
	white board
	wire
	
	--Erstellen von trrain.txt und valid.txt
	find data/yolo-aiv2/train -name '*.jpg' | sort > data/yolo-aiv2/train.txt
	find data/yolo-aiv2/valid -name '*.jpg' | sort > data/yolo-aiv2/valid.txt
	
	-Installieren CUDA (Laptop Grafikkarte Nvidia MX250)
	sudo apt update
	sudo apt install nvidia-cuda-toolkit
	nvcc --version
	(PC/Laptop Neustarten)
	
	-Anpassen Makefile (Kommentare Löschen)
	~/darknet: nano Makefile 
	GPU=1						//GPU aktivieren (Auf Pi alles zu GPU auf 0)
	CUDNN=0						//Wenn GPU CUDNN fähig dann auf 1
	CUDNN_HALF=0					//"
	OPENCV=1
	AVX=1
	OPENMP=1
	LIBSO=1
	ZED_CAMERA=0
	ZED_CAMERA_v2_8=0
	...
	-make clean
	-make -j$(nproc)

		
	-Erstellen eigener config Datei:
	~/darknet/cfg: nano yolov4-tiny-custom.cfg
	--yolov4-tiny-custom.cfg (Kommentare Löschen):
[net]
# Testing
#batch=1
#subdivisions=1
# Training
batch=64		(Wie viele Bilder werden Verarbeitet (hier 64))
subdivisions=32		(In wie vielen Prozessen werden diese Verarbeitet (hier 32) => 2Bilder pro durchlauf)
width=416
height=416
channels=3
momentum=0.9
decay=0.0005
angle=0
saturation = 1.5
exposure = 1.5
hue=.1

learning_rate=0.00261
burn_in=1000

max_batches = 2000200
policy=steps
steps=1600000,1800000
scales=.1,.1


#weights_reject_freq=1001
#ema_alpha=0.9998
#equidistant_point=1000
#num_sigmas_reject_badlabels=3
#badlabels_rejection_percentage=0.2


[convolutional]
batch_normalize=1
filters=32
size=3
stride=2
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=2
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1
groups=2
group_id=1

[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=leaky

[route]
layers = -1,-2

[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

[route]
layers = -6,-1

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1
groups=2
group_id=1

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[route]
layers = -1,-2

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[route]
layers = -6,-1

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

[route]
layers=-1
groups=2
group_id=1

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[route]
layers = -1,-2

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[route]
layers = -6,-1

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

##################################

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=105		([classes+5]*mask = [30+5]*3 = 35*3 = 105)
activation=linear



[yolo]
mask = 3,4,5
anchors = 10,14,  23,27,  37,58,  81,82,  135,169,  344,319
classes=30							(Anzahl muss zu Liste in obj.names passen und in obj.data muss die selbe anzahl stehen)
num=6
jitter=.3
scale_x_y = 1.05
cls_normalizer=1.0
iou_normalizer=0.07
iou_loss=ciou
ignore_thresh = .7
truth_thresh = 1
random=0
resize=1.5
nms_kind=greedynms
beta_nms=0.6
#new_coords=1
#scale_x_y = 2.0

[route]
layers = -4

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[upsample]
stride=2

[route]
layers = -1, 23

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=105		([classes+5]*mask = [30+5]*3 = 35*3 = 105)
activation=linear

[yolo]
mask=1,2,3
anchors=10,14, 23,27, 37,58, 81,82, 135,169, 344,319
classes=30							(Anzahl muss zu Liste in obj.names passen und in obj.data muss die selbe anzahl stehen)
num=6
jitter=.3
scale_x_y = 1.05
cls_normalizer=1.0
iou_normalizer=0.07
iou_loss=ciou
ignore_thresh = .7
truth_thresh = 1
random=0
resize=1.5
nms_kind=greedynms
beta_nms=0.6
#new_coords=1
#scale_x_y = 2.0


	-Training starten: 
	~/darknet: ./darknet detector train data/yolo-aiv2/obj.data cfg/yolov4-tiny-custom.cfg yolov4-tiny.conv.29 -map

	
----------------stand (05.06.2025 15:41)--------------------------------------------------------
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
